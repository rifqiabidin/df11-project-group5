### Stream Processing
=====================
- Prerequisite:

    A. Python 3: [Install Here](https://www.python.org/downloads/)
    
    B. Docker and Docker Compose: [Install Here](https://docs.docker.com/engine/install/ubuntu/)
    
    C. Google's Credential Service Account used for GCS and Bigquery access: [Get Here](https://developers.google.com/workspace/guides/create-credentials)
    
    D. Kafka Python Libraries, Install using `pip` package manager.
    
    Rename the google credentials into `google_credentials.json` and store it in your `$HOME` directory

- Project Guide:

    A. Clone this project
            
        git clone https://github.com/rifqiabidin/df11-project-group5.git
        
    B. Open this project in terminal and navigate to the directory of kafka_avro folder	
        
        # change directory
        cd src/streaming
        
    C. Install the required Kafka library, on terminal run following command

        pip install -r requirements.txt

    D. Set up a local Kafka cluster by creating images and containers using `docker-compose`

        docker compose up
	
	E. Start the data streaming by generating the messages using Kafka Producer

		python producer.py

    The outputs will be messages sent to the topic on the Kafka cluster

	F. While the Kafka Producer is running, open up new terminal and start the Kafka Consumer to read out the data from kafka topic

		python consumer.py
		
    Messages generated by the produced will start showing up in the consumer window
    
   ![kafka producer consumer](../../Kafka.jpg)
   ![kafka producer consumer](./Kafka.jpg)